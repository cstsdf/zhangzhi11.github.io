---
layout: post
tags:
  - 'deep learning'
title: Theano学习及探索
category: 'deep learning'
---
Theano是一个较早的深度学习框架，随着越来越多的深度学习框架的诞生，如：TensorFlow，MXnet等，Theano热度也有所下降,但是也不失为一个非常好的学习深度学习的一个工具。本文就是将笔者对theano的理解总结一下，同时帮助后来的人更好的学习，更深的理解请看官方教程。

我在以前的深度学习的工作中一直在用Keras，Keras是一个theano和TensorFlow的wrapper，直接提供上层的框架，可以较快的搭出一个深度学习模型，但是Keras也存在着一些问题，比如提供的预训练模型较少；速度较慢；由于封装的较为高级，如果更改模型内部的结构会较为麻烦。建议初学者可以先学习Keras了解深度学习，然后尝试多搭建几个模型感受一下深度学习的效果，然后再尝试使用Theano学习深度学习更加基础和本质的东西。
### 1. Theano基础
**Theano是什么？**Theano本身是一个通用的符号计算框架，它其实不仅可以用于深度学习，而且可以用于数值计算，线性规划等问题上。与传统的非符号计算架构不同，它首先将所有的变量或者数值表示为Tensor Variable的形式，然后利用计算图的形式将复杂的符号表达式编译为end-to-end的模型，然后才传入实际数据进行计算。不知道大家有没有用过一款类似于matlab的数值计算软件，mathematica，theano和它比较类似，都是先把整个表达式输入然后编译整个表达式，而不是一步一步求解计算。那这样做究竟有什么好处？主要是可以实现两个功能，表达式优化和自动求导。这在后面也会讲到。

**Theano的优缺点：**优点主要是实现使用图结构下的符号计算框架，对RNN支持很好，计算路径非常清晰，文档全面；缺点主要是偏底层（这时候就需要Keras了），调试困难，中间结构不能输出，编译时间长，预训练模型较少。

**Theano的结构**
Theano会把符号表达式转化为Graph，这种Graph主要由以下几个部分组成：
1. Variable：不论是变量还是像2,3,4这种常数还是矩阵形式的变量或者常量，在Theano中都表示为Tensor Variable的形式；
2. Op:基本的运算符号和操作，如加减乘除，三角函数，矩阵变换等；
3. Apply:把Variable和Op结合的部分。
举一个简单的例子：
```python
    import theano.tensor as T
    
    x = T.dmatrix('x')
    y = T.dmatrix('y')
    z = x + y
```
    
![apply](pic/apply.png)<br>
可以看出，Apply相当于是一个连接器，把input、output、Op联系在一起。这样做有两个好处，我会在后面的内容中介绍。

### 2. Theano语法学习
### 2.1 入门
首先举一个最简单的例子，计算两个数字的和。
```python
import numpy
import theano.tensor as T
from theano import function
x = T.dscalar('x')
y = T.dscalar('y')
z = x + y
f = function([x, y], z)
```
第1、2、3行分别导入numpy，theano.tensor和function，tensor就是张量，这个包中存储了所有tensor variable的数据格式，包括在第4，5行出现的scalar，还有vector，matrix，tensor3，tensor4等具体类型，他们的dim分别为0, 1, 2, 3, 4。
第4、5行定义了两个标量x， y，也就是上面说的tensor variable。第6行定义一个表达式，最后一行的function也是theano中最常用的函数，编译得到一个end-to-end的函数，中括号里的是输入，外的是输出。
最后调用这个函数就可以得到计算结果。
```python
>>>f(2, 3)
array(5.0)
```
#### 2.2 Shared
这也是一个tensor variable，名字虽然叫shared，但这不是它的本质，它本质上是一块稳定存在的存储空间，类似于全局变量，但与全局变量不同的是，它可以存储在显存中，很多场景下GPU加速效果明显。一个重要的语法特性是，Theano函数可以在每次执行时顺便修改shared variable的值。比如用shared variable实现累加操作：
```python
from theano import shared
state = shared(0)
inc = T.iscalar('inc')
accumulator = function([inc], state, updates=[(state,state+inc)])
```
state是一个shared variable，初始化为0，每次调用accumulator()，state都会加上inc。shared variable可以像普通tensor variable一样用于符号表达式，另外，他还有自己的值，可以直接用.get_value()和.set_value()方法来访问和修改。

上述代码还引入了一个东西：updates。这是用theano函数操作shared variable的方法，参数是一个list，list的每个元素都是二元tuple，tuple首元素是要修改的shared，第二个元素是更新后的值。updates中的share variable会在函数返回后更新自己的值。
#### 2.3 求导
tensor.grad(output,input)可以求表达式的导数，第一个参数是表达式输出，第二个是自变量或者自变量的list。
```python
import theano
import theano.tensor as TT

x = TT.dscalar('x')
y = x ** 2 # 定义函数y=x^2
gy = T.grad(y, x) # 导函数的表达式
f = theano.function([x], gy) # 将导函数编译成Theano函数
```
可以用`theano.pp(gy)`测试一下，可以看到
```python
>>> theano.pp(gy) # 打印优化之前的导函数表达式
((fill((x ** TensorConstant{2}), TensorConstant{1.0}) * TensorConstant{2}) * (x ** (TensorConstant{2} - TensorConstant{1})))

>>> f(4) # 带入数据计算x=4时的导数值
array(8.0)

>>> pp(f.maker.fgraph.outputs[0]) # 打印编译优化后的导函数表达式
(2.0 * x)
```
求解结果f'(4)=8，与预期相符。pp(gy)输出grad的结果，是未经优化的，有多个节点，编译后的函数f则是优化过的，只有一个节点，这也反映出，函数编译中的优化是很重要的。
#### 2.4 logistic 回归
接下来就直接进入实战了，不多说，直接上代码
```python
import numpy
import theano
import theano.tensor as T
rng = numpy.random

N = 400                                   # 训练集大小
feats = 784                               # 特征数量

# 生成数据集D
D = (rng.randn(N, feats), rng.randint(size=N, low=0, high=2))
training_steps = 10000

# 定义variable
x = T.dmatrix("x")
y = T.dvector("y")

# 初始化w，b
w = theano.shared(rng.randn(feats), name="w")
b = theano.shared(0., name="b")

print("Initial model:")
print(w.get_value())
print(b.get_value())

# 建立计算图
p_1 = 1 / (1 + T.exp(-T.dot(x, w) - b))   # 回归值
prediction = p_1 > 0.5                    # 分类结果
xent = -y * T.log(p_1) - (1-y) * T.log(1-p_1) # 交叉熵损失函数
cost = xent.mean() + 0.01 * (w ** 2).sum() # 对于整个数据集的损失函数，加入了L2正则项
gw, gb = T.grad(cost, [w, b])             # 计算cost对w，b的梯度

# 编译
train = theano.function(
          inputs=[x,y],
          outputs=[prediction, xent],
          updates=((w, w - 0.1 * gw), (b, b - 0.1 * gb)))
predict = theano.function(inputs=[x], outputs=prediction)

# 训练
for i in range(training_steps):
    pred, err = train(D[0], D[1])

print("Final model:")
print(w.get_value())
print(b.get_value())
print("target values for D:")
print(D[1])
print("prediction on D:")
print(predict(D[0]))
```
### 3. Theano计算图优化及自动求导
#### 3.1 计算图的优化
当调用theano.function时，编译优化就开始了。Theano会识别特定的一些pattern，替换成快速稳定但结果相同的图。Theano还会检测相同的子图，确保相同的子图不会被计算两次。还有就是类似于在手算的时候的对于公式的花间。总之，优化的过程会破坏原来我们编写的图结构，但会保持最终的函数输出相同。比如下面的例子：
```python
import theano

a = theano.tensor.vector("a")
b = a + a ** 10

f = theano.function([a], b)

print(f([0, 1, 2]))
[    0.     2.  1026.]
```
在[theano官方网站](http://deeplearning.net/software/theano/tutorial/ "theano官方网站")上也画出了优化前和优化后的图结构：
优化前：
![raw](pic/theano_raw.png)<br>

优化后：
![optimized](pic/theano_optimized.png)<br>

优化之后图结构面目全非，节点数少了很多，很多地方被Theano固有的composite模式代替了，当然，中间变量也完全不一样了，所以Theano在开启优化的情况下，自然无法像命令式架构的程序那样设断点debug，观察中间变量的值。
#### 3.2 自动求导
以前感觉这个真是一个很神奇的事情，往往手动求导时都不知道怎么求的问题在theano直接输入求导就好了，自从看了大神[colah的博客](http://colah.github.io/posts/2015-08-Backprop/ "colah的博客")，豁然开朗。
其实关键技术可以总结为：链式法则，求导的加法和乘法法则，反向求导还有动态规划。
例如，我们有这样一个表达式 e = (a + b) ( b + 1)。其包含三个操作：两个加法和一个乘法。为了更好的讲述，我们引入两个中间变量，c 和 d，这样每个函数的输出就有一个变量表示了。现在我们有：
c = a + b
d = b + 1
e = c  d
下面可以创建计算图了,然后设置各变量的值，通过这个图来计算每个节点的值。例如，假设`a = 2,b = 1`得到:
![jisuantu](pic/jisuantu.png)<br>
表达式的值就是6.
**那么如何求导呢？**我们需要和式法则（sum rule ）和 乘式法则（product rule）：
![add_mul](pic/add_mul.png)<br>
下面，在每条边上就有对应的导数了：
![jisuantu_grad](pic/jisuantu_grad.png)<br>
路径求和的法则其实就是 多元链式法则（multivariate chain rule）的 另一种思考方式。
**分解路径**
路径求和可能路径数量很容易就会组合爆炸。这时候就需要进行动态规划方法进行优化。
前向微分从图的输入开始，一步一步到达终点。在每个节点处，对输入的路径进行求和。每个这样的路径都表示输入影响该节点的一个部分。通过将这些影响加起来，我们就得到了输入影响该节点的全部，也就是关于输入的导数。
![forward_mode](pic/forward_mode.png)<br>
相对的，反向微分是从图的输出开始，反向一步一步抵达最开始输入处。在每个节点处，会合了所有源于该节点的路径。
![backward_mode](pic/backward_mode.png)<br>
一个拥有百万个输入和一个输出的函数。前向微分需要百万次遍历计算图才能得到最终的导数，而反向微分仅仅需要一次就能得到所有的导数！百万级的速度提升多么美妙！
训练神经网络时，我们将衡量神经网络表现的代价函数看做是那些决定网络行为的参数的函数。我们希望计算出代价函数关于所有参数的偏导数，从而进行梯度下降（gradient descent）。现在，常常会遇到百万甚至千万级的参数的神经网络。所以，反向微分，也就是 BP，在神经网络中发挥了关键作用！
其实计算图求导的优化主要就是通过反向微分和表达式简化来进行的。
### 参考
[theano官方网站](http://deeplearning.net/software/theano/tutorial/ "theano官方网站")<br>
[colah的博客](http://colah.github.io/posts/2015-08-Backprop/ "colah的博客")<br>
[李浩的博客](https://zhuanlan.zhihu.com/p/24218567 "李浩的博客")<br>
